{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2Z4jKZbEmSwy"
      },
      "outputs": [],
      "source": [
        "#Simulated gene expression data classification using Transformers\n",
        "#An example purely conceived of ChatGPT\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Generate synthetic gene expression data\n",
        "def generate_gene_expression_data(num_samples, num_genes):\n",
        "    data = np.random.rand(num_samples, num_genes)  # You can replace this with real gene expression data\n",
        "    labels = np.random.randint(0, 2, size=num_samples)  # Random binary labels for demonstration\n",
        "    return data, labels\n",
        "\n",
        "# Create the synthetic gene expression dataset\n",
        "num_samples = 1000\n",
        "num_genes = 50\n",
        "num_classes = 2\n",
        "\n",
        "data, labels = generate_gene_expression_data(num_samples, num_genes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def transformer_model(input_shape, num_classes, d_model=128, num_heads=4, ff_dim=128, dropout=0.1):\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = inputs\n",
        "\n",
        "    # Positional encoding\n",
        "    positions = tf.range(start=0, limit=input_shape[1], delta=1)\n",
        "    positional_encoding = 1 / tf.pow(10000, 2 * tf.cast(tf.range(d_model // 2), tf.float32) / d_model)\n",
        "    positional_encoding = tf.expand_dims(positional_encoding, 0)\n",
        "    positional_encoding = tf.expand_dims(positional_encoding, 1)\n",
        "    position_indices = tf.expand_dims(positions, 1)\n",
        "    positional_encoding = tf.matmul(tf.cast(position_indices, tf.float32), positional_encoding)  # Cast to float32\n",
        "    positional_encoding = tf.concat([tf.cos(positional_encoding), tf.sin(positional_encoding)], axis=-1)\n",
        "    x += positional_encoding\n",
        "\n",
        "    # Transformer Encoder\n",
        "    for _ in range(num_heads):\n",
        "        # Multi-head self-attention mechanism\n",
        "        attention_out = tf.keras.layers.MultiHeadAttention(key_dim=d_model // num_heads, num_heads=num_heads, dropout=dropout)(x, x)\n",
        "        # Add and normalize\n",
        "        x = tf.keras.layers.Add()([attention_out, x])\n",
        "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "        # Feed Forward Part\n",
        "        ffn = tf.keras.Sequential([\n",
        "            Dense(ff_dim, activation='relu'),\n",
        "            Dense(d_model),\n",
        "        ])\n",
        "        ffn_out = ffn(x)\n",
        "        # Add and normalize\n",
        "        x = tf.keras.layers.Add()([ffn_out, x])\n",
        "        x = tf.keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
        "\n",
        "    # Global average pooling\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D(data_format='channels_first')(x)\n",
        "    x = Dropout(dropout)(x)\n",
        "\n",
        "    outputs = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    return model\n",
        "\n",
        "model = transformer_model((num_genes, 1), num_classes)"
      ],
      "metadata": {
        "id": "Ng05Q49PmbST"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "split_ratio = 0.8\n",
        "split_idx = int(num_samples * split_ratio)\n",
        "\n",
        "x_train, y_train = data[:split_idx], labels[:split_idx]\n",
        "x_test, y_test = data[split_idx:], labels[split_idx:]\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(lr=1e-4), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ejc-HbEDmrq4",
        "outputId": "8123a3a3-751b-486b-a30a-d8f911205a2a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "20/20 [==============================] - 14s 39ms/step - loss: 0.6931 - accuracy: 0.5188 - val_loss: 0.6910 - val_accuracy: 0.5688\n",
            "Epoch 2/10\n",
            "20/20 [==============================] - 0s 14ms/step - loss: 0.6928 - accuracy: 0.5188 - val_loss: 0.6905 - val_accuracy: 0.5688\n",
            "Epoch 3/10\n",
            "20/20 [==============================] - 0s 16ms/step - loss: 0.6930 - accuracy: 0.5188 - val_loss: 0.6896 - val_accuracy: 0.5688\n",
            "Epoch 4/10\n",
            "20/20 [==============================] - 0s 15ms/step - loss: 0.6927 - accuracy: 0.5188 - val_loss: 0.6886 - val_accuracy: 0.5688\n",
            "Epoch 5/10\n",
            "20/20 [==============================] - 0s 15ms/step - loss: 0.6929 - accuracy: 0.5188 - val_loss: 0.6898 - val_accuracy: 0.5688\n",
            "Epoch 6/10\n",
            "20/20 [==============================] - 0s 16ms/step - loss: 0.6925 - accuracy: 0.5188 - val_loss: 0.6897 - val_accuracy: 0.5688\n",
            "Epoch 7/10\n",
            "20/20 [==============================] - 0s 16ms/step - loss: 0.6928 - accuracy: 0.5188 - val_loss: 0.6901 - val_accuracy: 0.5688\n",
            "Epoch 8/10\n",
            "20/20 [==============================] - 0s 17ms/step - loss: 0.6930 - accuracy: 0.5188 - val_loss: 0.6900 - val_accuracy: 0.5688\n",
            "Epoch 9/10\n",
            "20/20 [==============================] - 0s 16ms/step - loss: 0.6925 - accuracy: 0.5188 - val_loss: 0.6888 - val_accuracy: 0.5688\n",
            "Epoch 10/10\n",
            "20/20 [==============================] - 0s 15ms/step - loss: 0.6923 - accuracy: 0.5188 - val_loss: 0.6883 - val_accuracy: 0.5688\n",
            "7/7 [==============================] - 1s 11ms/step - loss: 0.6973 - accuracy: 0.4600\n",
            "Test Loss: 0.6973, Test Accuracy: 0.4600\n"
          ]
        }
      ]
    }
  ]
}